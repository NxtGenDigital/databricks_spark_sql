{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Querying Data Lakes with SQL\n\nApache Spark&trade; and Databricks&reg; make it easy to work with hierarchical data, such as nested JSON records.\n\nPerform exploratory data analysis (EDA) to gain insights from a Data Lake.\n\n## In this lesson you:\n* Use SQL to query a Data Lake\n* Clean messy data sets\n* Join two cleaned data sets\n\n## Audience\n* Primary Audience: Data Analysts\n* Additional Audiences: Data Engineers and Data Scientists\n\n## Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 6.3**\n* Familiarity with <a href=\"https://www.w3schools.com/sql/\" target=\"_blank\">ANSI SQL</a> is required"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup & Classroom-Cleanup<br>\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell) and the **`Classroom-Cleanup`** cell at the end of each lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Data Lakes\n\nCompanies frequently have thousands of large data files gathered from various teams and departments, typically using a diverse variety of formats including CSV, JSON, and XML.  Analysts often wish to extract insights from this data.\n\nThe classic approach to querying this data is to load it into a central database called a Data Warehouse.  This involves the time-consuming operation of designing the schema for the central database, extracting the data from the various data sources, transforming the data to fit the warehouse schema, and loading it into the central database.  The analyst can then query this enterprise warehouse directly or query smaller data marts created to optimize specific types of queries.\n\nThis classic Data Warehouse approach works well but requires a great deal of upfront effort to design and populate schemas.  It also limits historical data, which is restrained to only the data that fits the warehouse’s schema.\n\nAn alternative to this approach is the Data Lake.  A _Data Lake_:\n\n* Is a storage repository that cheaply stores a vast amount of raw data in its native format\n* Consists of current and historical data dumps in various formats including XML, JSON, CSV, Parquet, etc.\n* Also may contain operational relational databases with live transactional data\n\nSpark is ideal for querying Data Lakes as the Spark SQL query engine is capable of reading directly from the raw files and then executing SQL queries to join and aggregate the Data.\n\nYou will see in this lesson that once two tables are created (independent of their underlying file type), we can join them, execute nested queries, and perform other operations across our Data Lake."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/yyblq4fgfl?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/yyblq4fgfl?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["## Looking at our Data Lake\n\nYou can start by reviewing which files are in our Data Lake.\n\nIn `dbfs:/mnt/training/crime-data-2016`, there are some Parquet files containing 2016 crime data from several United States cities.\n\nAs you can see in the cell below, we have data for Boston, Chicago, New Orleans, and more."],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/6v5a6qgfbb?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/6v5a6qgfbb?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/training/crime-data-2016"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["The next step in looking at the data is to create a temporary view for each file.  Recall that temporary views use a similar syntax to `CREATE TABLE` but using the command `CREATE TEMPORARY VIEW`.  Temporary views are removed once your session has ended while tables are persisted beyond a given session.\n\nStart by creating a view of the data from New York and then Boston:\n\n| City          | Table Name              | Path to DBFS file\n| ------------- | ----------------------- | -----------------\n| **New York**  | `CrimeDataNewYork`      | `dbfs:/mnt/training/crime-data-2016/Crime-Data-New-York-2016.parquet`\n| **Boston**    | `CrimeDataBoston`       | `dbfs:/mnt/training/crime-data-2016/Crime-Data-Boston-2016.parquet`"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW CrimeDataNewYork\n  USING parquet\n  OPTIONS (\n    path \"dbfs:/mnt/training/crime-data-2016/Crime-Data-New-York-2016.parquet\"\n  )"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW CrimeDataBoston\n  USING parquet\n  OPTIONS (\n    path \"dbfs:/mnt/training/crime-data-2016/Crime-Data-Boston-2016.parquet\"\n  )"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["With the view created, it is now possible to review the first couple records of each file.\n\nNotice in the example below:\n* The `CrimeDataNewYork` and `CrimeDataBoston` datasets use different names for the columns\n* The data itself is formatted differently and different names are used for similar concepts\n\nThis is common in a Data Lake.  Often files are added to a Data Lake by different groups at different times.  While each file itself usually has clean data, there is little consistency across files.  The advantage of this strategy is that anyone can contribute information to the Data Lake and that Data Lakes scale to store arbitrarily large and diverse data.  The tradeoff for this ease in storing data is that it doesn’t have the rigid structure of a more traditional relational data model so the person querying the Data Lake will need to clean the data before extracting useful insights.\n\nThe alternative to a Data Lake is a Data Warehouse.  In a Data Warehouse, a committee often regulates the schema and ensures data is cleaned before being made available.  This makes querying much easier but also makes gathering the data much more expensive and time-consuming.  Many companies choose to start with a Data Lake to accumulate data.  Then, as the need arises, they clean the data and produce higher quality tables for querying.  This reduces the upfront costs while still making data easier to query over time.  These cleaned tables can even be later loaded into a formal data warehouse through nightly batch jobs.  In this way, Apache Spark can be used to manage and query both Data Lakes and Data Warehouses."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM CrimeDataNewYork"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql\n\nSELECT * FROM CrimeDataBoston"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Same type of data, different structure\n\nIn this section, we examine crime data to figure out how to extract homicide statistics.\n\nBecause our data sets are pooled together in a Data Lake, each city may use different field names and values to indicate homicides, dates, etc.\n\nFor example:\n* Some cities use the value \"HOMICIDE\", \"CRIMINAL HOMICIDE\" or even \"MURDER\"\n* In New York, the column is named `offenseDescription` but, in Boston, the column is named `OFFENSE_CODE_GROUP`\n* In New York, the date of the event is in the `reportDate` column but, in Boston, there is a single column named `MONTH`"],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/9mc9dtyx5u?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/9mc9dtyx5u?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\nTo get started, create a temporary view containing only the homicide-related rows.\n\nAt the same time, normalize the data structure of each table so that all the columns (and their values) line up with each other.\n\nIn the case of New York and Boston, here are the unique characteristics of each data set:\n\n| | Offense-Column        | Offense-Value          | Reported-Column  | Reported-Data Type |\n|-|-----------------------|------------------------|-----------------------------------|\n| New York | `offenseDescription`  | starts with \"murder\" or \"homicide\" | `reportDate`     | `timestamp`    |\n| Boston | `OFFENSE_CODE_GROUP`  | \"Homicide\"             | `MONTH`          | `integer`      |\n\nFor the upcoming aggregation, you will need to alter the New York data set to include a `month` column which can be computed from the `reportDate` column using the `month()` function. Boston already has this column.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> One helpful tool for finding the offences we're looking for is using <a href=\"https://en.wikipedia.org/wiki/Regular_expression\" target=\"_blank\">regular expressions</a> supported by SQL\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We can also normalize the values with the `CASE`, `WHEN`, `THEN` & `ELSE` expressions but that is not required for the task at hand."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW HomicidesNewYork AS\n  SELECT month(reportDate) AS month, offenseDescription AS offense\n  FROM CrimeDataNewYork\n  WHERE lower(offenseDescription) LIKE 'murder%' OR lower(offenseDescription) LIKE 'homicide%'"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW HomicidesBoston AS\n  SELECT month, OFFENSE_CODE_GROUP AS offense\n  FROM CrimeDataBoston\n  WHERE lower(OFFENSE_CODE_GROUP) = 'homicide'"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["You can see below that the structure of our two tables is now identical."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM HomicidesNewYork LIMIT 5"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%sql\n\nSELECT * FROM HomicidesBoston LIMIT 5"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Analyzing our data"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nNow that we have normalized the homicide data for each city we can combine the two by taking their union.\n\nWhen we are done, we can then aggregate that data to compute the number of homicides per month.\n\nStart by creating a new view called `HomicidesBostonAndNewYork` which simply unions the result of two `SELECT` statements together.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://stackoverflow.com/questions/49925/what-is-the-difference-between-union-and-union-all\">this Stack Overflow post</a> for the difference between `UNION` and `UNION ALL`"],"metadata":{}},{"cell_type":"markdown","source":["<iframe  \nsrc=\"//fast.wistia.net/embed/iframe/ld3fh1x0ig?videoFoam=true\"\nstyle=\"border:1px solid #1cb1c2;\"\nallowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\nname=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\noallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n<div>\n<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/ld3fh1x0ig?seo=false\">\n  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n</div>"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW HomicidesBostonAndNewYork AS\n  SELECT * FROM HomicidesNewYork\n    UNION ALL\n  SELECT * FROM HomicidesBoston"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["You can now see below all the data in one table:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT *\nFROM HomicidesBostonAndNewYork\nORDER BY month"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["And finally we can perform a simple aggregation to see the number of homicides per month:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT month, count(*) AS homicides\nFROM HomicidesBostonAndNewYork\nGROUP BY month\nORDER BY month"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Exercise 1\n\nMerge the crime data for Chicago with the data for New York and Boston and then update our final aggregation of counts-by-month."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nCreate the initial view of the Chicago data.\n0. The source file is `dbfs:/mnt/training/crime-data-2016/Crime-Data-Chicago-2016.parquet`\n0. Name the view `CrimeDataChicago`\n0. View the data with a simple `SELECT` statement"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\ntotal = spark.sql(\"select count(*) from CrimeDataChicago\").first()[0]\ndbTest(\"SQL-L6-crimeDataChicago-count\", 267872, total)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["-sandbox\n### Step 2\n\nCreate a new view that normalizes the data structure.\n0. Name the view `HomicidesChicago`\n0. The table should have at least two columns: `month` and `offense`\n0. Filter the data to only include homicides\n0. View the data with a simple `SELECT` statement\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You will need to use the `month()` function to extract the month-of-the-year.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** To find out which values for each offense constitutes a homicide, produce a distinct list of values from the table `CrimeDataChicago`."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nhomicidesChicago = spark.sql(\"SELECT month, count(*) FROM HomicidesChicago GROUP BY month ORDER BY month\").collect()\ndbTest(\"SQL-L6-homicideChicago-len\", 12, len(homicidesChicago))\n\ndbTest(\"SQL-L6-homicideChicago-0\", 54, homicidesChicago[0][1])\ndbTest(\"SQL-L6-homicideChicago-6\", 71, homicidesChicago[6][1])\ndbTest(\"SQL-L6-homicideChicago-11\", 58, homicidesChicago[11][1])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["-sandbox\n### Step 3\n\nCreate a new view that merges all three data sets (New York, Boston, Chicago):\n0. Name the view `AllHomicides`\n0. Use the `UNION ALL` expression introduced earlier to merge all three tables\n  * `HomicidesNewYork`\n  * `HomicidesBoston`\n  * `HomicidesChicago`\n0. View the data with a simple `SELECT` statement\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** To union three tables together, copy the previous example and just add as second `UNION` statement followed by the appropriate `SELECT` statement."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nallHomicides = spark.sql(\"SELECT count(*) AS total FROM AllHomicides\").first()[0]\ndbTest(\"SQL-L6-allHomicides-count\", 1203, allHomicides)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### Step 4\n\nCreate a new view that counts the number of homicides per month.\n0. Name the view `HomicidesByMonth`\n0. Rename the column `count(1)` to `homicides`\n0. Group the data by `month`\n0. Sort the data by `month`\n0. Count the number of records for each aggregate\n0. View the data with a simple `SELECT` statement"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nallHomicides = spark.sql(\"SELECT * FROM HomicidesByMonth\").collect()\ndbTest(\"SQL-L6-homicidesByMonth-len\", 12, len(allHomicides))\n\ndbTest(\"SQL-L6-homicidesByMonth-0\", 1, allHomicides[0].month)\ndbTest(\"SQL-L6-homicidesByMonth-11\", 12, allHomicides[11].month)\n\ndbTest(\"SQL-L6-allHomicides-0\", 83, allHomicides[0].homicides)\ndbTest(\"SQL-L6-allHomicides-1\", 68, allHomicides[1].homicides)\ndbTest(\"SQL-L6-allHomicides-2\", 72, allHomicides[2].homicides)\ndbTest(\"SQL-L6-allHomicides-3\", 76, allHomicides[3].homicides)\ndbTest(\"SQL-L6-allHomicides-4\", 105, allHomicides[4].homicides)\ndbTest(\"SQL-L6-allHomicides-5\", 120, allHomicides[5].homicides)\ndbTest(\"SQL-L6-allHomicides-6\", 116, allHomicides[6].homicides)\ndbTest(\"SQL-L6-allHomicides-7\", 144, allHomicides[7].homicides)\ndbTest(\"SQL-L6-allHomicides-8\", 109, allHomicides[8].homicides)\ndbTest(\"SQL-L6-allHomicides-9\", 109, allHomicides[9].homicides)\ndbTest(\"SQL-L6-allHomicides-10\", 111, allHomicides[10].homicides)\ndbTest(\"SQL-L6-allHomicides-11\", 90, allHomicides[11].homicides)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["## Summary\n\n* Spark SQL allows you to easily manipulate data in a Data Lake\n* Temporary views help to save your cleaned data for downstream analysis"],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is a Data Lake?  \n**A:** Data Lakes are a loose collection of data files gathered from various sources.  Spark loads each file as a table and then executes queries joining and aggregating these files.\n\n**Q:** What are some advantages of Data Lakes over more classic Data Warehouses?  \n**A:** Data Lakes allow for large amounts of data to be aggregated from many sources with minimal ceremony or overhead.  Data Lakes also allow for very very large files.  Powerful query engines such as Spark can read the diverse collection of files and execute complex queries efficiently.\n\n**Q:** What are some advantages of Data Warehouses?  \n**A:** Data warehouses are neatly curated to ensure data from all sources fit a common schema.  This makes them very easy to query.\n\n**Q:** What's the best way to combine the advantages of Data Lakes and Data Warehouses?  \n**A:** Start with a Data Lake.  As you query, you will discover cases where the data needs to be cleaned, combined, and made more accessible.  Create periodic Spark jobs to read these raw sources and write new \"golden\" tables that are cleaned and more easily queried."],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Cleanup<br>\n\nRun the **`Classroom-Cleanup`** cell below to remove any artifacts created by this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Cleanup\""],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> All done!</h2>\n\nThank you for your participation!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"SSQL 06 - Data Lakes","notebookId":2451692899067632},"nbformat":4,"nbformat_minor":0}
